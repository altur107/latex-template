%% Aide-mémoire
\documentclass[10pt, french, landscape]{article}
%% -----------------------------
%% Préambule
%% -----------------------------

\def\cours{Processus stochastique}
\def\sigle{ACT-2009}
\def\session{Automne 2018}
\def\auteur{Gabriel Crepeault-Cauchon / Nicholas Langevin}
\def\BackgroundColor{white}
\def\SectionColor{blue!80!white}
\def\SubSectionColor{blue!30!black}
\input{../../preamble-template/cheatsht-preamble.tex}

%% -----------------------------
%% Début du document
%% -----------------------------
\begin{document}

\small
\begin{multicols*}{3} % Nombre de colonnes (peut être changé plus tard.)
\setcounter{section}{2}

\section{Processus de Poisson}
\subsection*{Processus de Poisson non-homogène}
\begin{definition}[Définition]
Un processus de dénombrement $\{ N(t) ; t \geq 0 \}$ est dit être un processus de Poisson non-homogène avec fonction d'intensité $\lambda(t)$ si
\begin{enumerate}[label=(\arabic*)]
\item $N(0) = 0$ ;
\item $\{ N(t) ; t \geq 0 \}$ a des accroissements indépendants ;
\item $\prob{N(t+h) - N(t) = 1} = \lambda(t) h + o(h)$ ;
\item $\prob{N(t+h) - N(t) \geq 2} = o(h)$ où $o(h)$ est une fonction négligeable.
\end{enumerate}
\end{definition}

\subsubsection*{Proposition 1}
\[\prob{N(t+s) - N(t) = n}  = \frac{\left(m(t+s) - m(s) \right)^n}{n!} e^{-(m(t+s) - m(s))} \]
où $m(t) = \int_{0}^{t} \lambda(x) dx$. On a alors que
\[N(t+s) - N(s) \sim Pois(m(t+s) - m(s))\]


\subsubsection*{Proposition 2}
Si $S_n$ désigne le temps d'occurence du $n$\up{e} évènement, alors
\[f_{S_n}(t) = \lambda(t) \frac{m(t)^{n-1}}{(n-1)!} e^{-m(t)} \]

\subsubsection*{Proposition 3}
Si $T_n = S_{n} - S_{n-1}$, alors on a, pour $n \geq 2$,
\[f_{T_n}(t) = \frac{1}{(n-2)!} \int_{0}^{\infty} \lambda(s) \lambda(t+s) m(s)^{n-2} e^{-m(t+s)} ds \]

\columnbreak % forcer le changement de colonne
\subsection*{Processus de Poisson composé}
\begin{definition}[Définition]
Un processus stochastique $\{ N(t) ; t \geq 0  \}$ est dit être un processus de Poisson composé s'il peut être représenté comme suit : 
\[X(t) = \sum_{i=1}^{N(t)} Y_i\]
où $\{ N(t) ; t \geq 0  \}$ est un Processus de Poisson avec paramètre $\lambda > 0$ et $\{ Y_i ; i \in \naturels \}$ est une suite de v.a. \emph{iid} indépendantes de $N(t)$.
\end{definition}

\subsubsection*{Proposition 1}
Soit $\{ X(t) ; t \geq 0 \}$ un processus de Poisson composé avec paramètre $\lambda > 0$ et supposons que $\prob{Y_i = \alpha_j} = p_j$, $\sum p_j = 1$. Alors,
\[X(t) = \sum_j \alpha_j N_j(t) \]
où $N_j(t)$ est le nombre de fois que se produit l'évènement $\alpha_j$ dans l'intervalle de temps $[0,t]$, et $\{N(t) ; t \geq 0  \}$ forme une suite de v.a. indépentantes telles que $N_j(t) \sim Pois(\lambda p_j t$. \\
Lorsque $t \to \infty$, alors $X(t)$ est asymptotiquement normal, i.e.
\[X(t) \sim \mathcal{N}\left( \lambda t \esp{Y}, \lambda t \esp{Y^2} \right)\]

\subsubsection*{Proposition 2}
Si $\{ X(t) ; t \geq 0 \}$ et $\{ Y(t) ; t \geq 0 \}$ sont 2 processus de Poisson composés indépendants avec paramètres et fonctions de répartition $\lambda_1, F_{X_1}$ et $\lambda_2, F_{Y_1}$ respectivement, alors $\{ X(t) + Y(t) ; t \geq 0 \}$ est aussi un processus de Poisson composé avec paramètre $\lambda_1  \lambda_2$ et fonction de répartition $F_{X_1 + Y_1}$ telle que
\[F_{X_1 + Y_1} = \frac{\lambda_1 F_{X_1} + \lambda_2 F_{Y_1}}{\lambda_1 + \lambda_2}  \]


\columnbreak % forcer le changement de colonne
\subsection*{Processus de Poisson conditionnel}
\begin{definition}[Définition]
Un processus de dénombrement avec un taux aléatoire $\Lambda > 0$ est un processus de Poisson conditionnel si $\{ N(t) | \Lambda = \lambda ; t \geq 0 \}$ est un processus de Poisson avec taux $\lambda > 0$.
\end{definition}

\subsubsection*{Rappel sur la loi Gamma}
La fonction de répartition de la loi Gamma, lorsque $\alpha \in \entiers$, est définie par
\[F_X(x) = 1 - \sum_{k=1}^{\alpha-1} \frac{(\lambda x)^k e^{-\lambda x}}{k!}\]


\subsubsection*{Remarques importantes}
\begin{enumerate}[label=(\arabic*)]
\item Un processus de Poisson conditionnel a des accroissements stationnaires (i.e. l'accroissement ne dépend pas d'où on est, mais plutôt de l'intervalle de temps) ; 
\item Mais le processus de Poisson conditionnel n'a pas nécessairement des accroissements indépendants ;
\item Identité Poisson-Gamma : si on a $\Lambda \sim \Gamma(m, \theta)$, alors\footnote{Être capable de faire cette démonstration pour l'examen}
\[N(t) \sim NB\left(r = m, p = \frac{\theta}{\theta + t} \right) \]

\item L'espérance et la variance d'un processus de Poisson conditionnel sont définies par
\begin{align*}
\esp{N(t)} & = t \esp{\Lambda} \\
\variance{N(t)} & =  t \esp{\Lambda} + t^2 \variance{\Lambda}
\end{align*}

\item En utilisant le théorème de Bayes, on peut trouver la fonction de répartition $F_{\Lambda | N(t)}(x | n)$ et fonction de densité $f_{\Lambda | N(t)}(x | n)$ telles que
\begin{align*}
F_{\Lambda | N(t)}(x | n)	& = \frac{\prob{\Lambda \leq x | N(t) = n}}{\prob{N(t) = n}} \\
& = \frac{\prob{N(t) = n | \Lambda} f_\Lambda(\lambda) d \lambda}{\int_{0}^{\infty} \prob{N(t) = n | \Lambda = \lambda} f_\Lambda(\lambda) d\lambda} \\
\end{align*}

\item On a, $\forall t > 0$,
\begin{align*}
\prob{N(t) > n} = \int_{0}^{\infty} \overline{F}_{\Lambda}\left( \frac{x}{n} \right) \frac{x^n}{n!} e^{-x} dx
\end{align*}
\end{enumerate}

\section{Processus de renouvellement}
\subsection*{Définitions générales}
\begin{itemize}
\item $T_n$ : intervalle de temps entre le $(n-1)$\up{e} et le $n$\up{e} renouvellement ;
\item $S_n = \sum_{i=1}^{n} T_i$ : le temps d'occurence du $n$\up{e} renouvellement. On va souvent noter $S_{N(t)}$, avec $N(t)$ comme temps d'arrêt du processus\footnote{$N(t)$ est le temps d'arrêt dans le sens où on cesse le processus de dénombrement lorsqu'on atteint $N(t)$.} ;

\item $\mu = \esp{T_i}$ : temps moyen d'attente entre 2 renouvellements ;
\end{itemize}

\subsection*{Distribution de $N(t)$}
On définit $N(t)$ comme $N(t) = \max \{ n : S_n \leq t \}$. Alors,
\begin{align*}
\prob{N(t) = n} = F_T^{\ast n}(t) - F_T^{\ast(n+1)}(t)
\end{align*}
Dans le cas où $T \sim Erlang(m, \lambda)$, alors
\[\prob{N(t) = n}  = \sum_{k=mn}^{m(n+1) - 1} \frac{(\lambda x)^k e^{-\lambda x}}{k!} \]

\subsection*{Fonction de renouvellement}
La fonction de renouvellement est le nombre moyen d'occurences dans l'intervalle $[0,t]$  :
\[m(t) = \esp{N(t)} = \sum_{n=1}^{\infty} F_T^{\ast(n)}(t)  \]

\subsubsection*{Solution de l'équation de renouvellement}
$m(t)$ satisfait \emph{l'équation de renouvellement}, soit
\[m(t) = F_T(t) + \int_{0}^{t} m(t-x) f_T(x) dx  \]

\subsubsection*{Relation biunivoque entre $m(t)$ et $F_T$}
Avec la transformée de Laplace de $m(t)$, $\hat{m}(s)$, on a
\begin{align*}
\hat{m}(s) &  = \frac{\hat{f}_T(s)}{s} + \hat{m}(s) \hat{f}_T(s) \\
& = \frac{\hat{f}(s)}{s\left(1 - \hat{f}(s)    \right)}
\end{align*}


\subsection*{Théorèmes limites}
\begin{enumerate}[label=(\arabic*)]
\item On a que $N(\infty) = \infty$ avec probabilité 1. De plus,
\begin{align*}
\frac{N(t)}{t} \underset{t \to \infty}{\longrightarrow} \frac{1}{\mu}
\end{align*}
avec une probabilité \emph{presque certaine}.

\item \emph{Théorème élémentaire du renouvellement} : avec $t \to \infty$, on a
\[\frac{m(t)}{t} \underset{t \to \infty}{\longrightarrow} \frac{1}{\mu} \]

\item Lorsque $t \to \infty$, $N(t)$ est aymptotiquement normale, telle que
%\begin{align*} % version sous forme de \Phi
%\frac{N(t) - \frac{t}{\esp{T}}}{\sqrt{\frac{t \variance{T}}{\esp{T}^3}}} \sim \mathcal{N} (0,1)
%\end{align*}

\begin{flalign*} % version 2
N(t) \sim \mathcal{N} \left( \frac{t}{\esp{T}},  \frac{t \variance{T}}{\esp{T}^3}   \right)
\end{flalign*}
\end{enumerate}

\subsection*{Équation de renouvellement}
De façon générale, si on a une équation intégrale d'une fonction $g(t)$ telle que
\[g(t) = h(t) + \int_{0}^{t} g(t-x) dF_T(x) \]
Alors, la seule solution est
\[g(t) = h(t) + \int_{0}^{t} h(t-x) d m(x) \]

\subsection*{Distribution de $S_{N(t)}$}
On peut définir la fonction de répartition et l'espérance de $S_{N(t)}$ comme
\[F_{S_{N(t)}}(x) = \overline{F}_T(t) + \int_{0}^{x} \overline{F}_T(t-y) dm(y)  \]
et
\[\esp{S_{N(t)}}  = t F_T(t) - \int_{0}^{t} (t-y)\overline{F}_T(t-y) d m(y) \]
De plus,
\[\esp{S_{N(t) + 1}} = \esp{T}(m(t) + 1)\]

\subsection*{Key renewal theorem}
\[\lim_{t \to \infty} \int_{0}^{t} h(t-x) d m(x) = \frac{1}{\esp{T}} \int_{0}^{\infty} h(x) dx \]

\subsection*{Processus de renouvellement avec délai}
\begin{itemize}
\item Soit $\{ T_n : n \in \naturels \}$ des temps entre des renouvellements succesifs qui sont \emph{iid} tel que $F_{T_n}(t) = F_{T_2}(t)$ pour $n \geq 2$ et $F_{T_1(t)} \neq F_{T_2}(t)$. Alors $\{N_d(t) ; t \geq 0 \}$ est dit être un processus de renouvellement avec délai. 

\item La distribution de $N_d(t)$ est
\[\prob{N_d(t) = n} = F_{T_1} \ast F_{T_2}^{\ast(n-1)}(t) - F_{T_1} \ast F_{T_2}^{\ast (n)}(t)  \]

\item la fonction de renouvellement $m_d(t)$ est donc
\[m_d(t) = \sum_{n=1}^{\infty} F_{T_1} \ast F_{T_2}^{\ast (n-1)}(t)  \]

\item De plus, $m_d(t)$ satisfait aussi l'équation de renouvellement, telle que
\[m_d(t) = F_{T_1}(t) + \int_{0}^{t}  m_o(t-x) f_{T_1}(x) dx \]
où $m_o(t)$ est la fonction de renouvellement d'un processus de renouvellement ordinaire qui débute à $T_2$.
\end{itemize}

\subsection*{Processus de renouvellement \emph{stationnaire}}
\begin{itemize}
\item Un processus de renouvellement $\{ N_e(t) ; t \geq 0 \}$ est dit stationnaire si
\[F_{T_1} = F_e(t) = \frac{\int_{0}^{t} \overline{F}_{T_2}(x) dx  }{\esp{T_2}}\]

\item La fonction de renouvellement $m_e(t)$ est définie par
\[m_e(t) = \esp{N_e(t)} = \frac{t}{\esp{T_2}} \]

\item La distribution de $N_e(t)$ est définie par
\[\prob{N_e(t+h) - N_e(t) = n} = \prob{N_e(h) = n}\]
Car les accroissements sont stationnaires.
\end{itemize}

\subsection*{Processus de renouvellement alterné}
\begin{itemize}
\item Soit la suite $\{ (T_n, T_n') ; n \in \naturels \}$ des vecteurs \emph{iid} où les composantes $(T_n, T_n')$ peuvent être dépendante. $T_n$ représente un intervalle de temps dans lequel le processus (de renouvellement) est \emph{on} et $T_n'$ un intervalle de temps où le processus est \emph{off}.

\item On peut donc définir 2 processus  (\emph{on} et \emph{off}) :  
\begin{itemize}
	\item $\{ N_1(t) ; t \geq 0 \}$ est un processus de renouvellement \emph{avec délai} généré par la suite des temps $\{ T_1, T_n' + T_{n+1} ; n \in \entiers \}$, et sa fonction de renouvellement est
	\begin{align*}
	m_1(t) & = \sum_{n=1}^{\infty} F_{T_1} \ast F_{T_2 + T_1}
	^{\ast(n-1)}(t) \\
	& = \sum_{n=1}^{\infty} F_{T_1}^{\ast(n)}(t) \ast F_{T_1'}^{\ast(n-1)}(t) \\
	\end{align*}
	\item 	$\{ N_2(t) ; t \geq 0 \}$ est un processus de renouvellement \emph{ordinaire} généré par la suite des temps $\{ T_n + T_n'; n \in \entiers \}$, et sa fonction de renouvellement est
	\begin{align*}
	m_2(t) &= \sum_{n=1}^{\infty} F_{T_1  T_1'}^{\ast(n)}(t) = \sum_{n=1}^{\infty} F_{T_1}^{\ast(n)} \ast F_{T_1'}^{\ast(n)}(t) \\
	\end{align*}
\end{itemize}
\item \textbf{Proposition 1 : } Supposons que $T_n$ est indépendant de $T_n'$, $\forall n \in \naturels$ et soit $p_i(t)$ la probabilité que le processus de renouvellement alterné soit dans l'état $i$ au temps $t$, $i=1,2$. Alors,
\[p_1(t) = m_2(t) - m_1(t) + 1 = 1 - p_2(t) \]

\item \textbf{Proposition 2 : } Avec les mêmes hypothèses qu'à la proposition 1, on a
\[\lim_{t \to \infty} p_1(t) = \frac{\esp{T_1}}{\esp{T_1} + \esp{T_1'}} = 1 - \lim_{t \to \infty} p_2(t) \]
\end{itemize}

\subsection*{Application : somme de renouvellements avec réclamations escomptées}
\begin{itemize}
\item On considère le processus des réclamations escomptées à $t=0$, soit $\{ Z(t) ; t \geq 0 \}$, défini par
\[Z(t) = \sum_{k=1}^{N(t)} e^{-\delta S_k} X_k  \]
où
\begin{itemize}
	\item $\{N(t) ; t \geq 0 \}$ un processus de renouvellement ordinaire ;
	\item $S_k$ est le moment où se produit la $k$\up{e} réclamation ;
	\item La suite $\{ X_n ; n \in \entiers \}$ de v.a. \emph{iid} et indépendantes de $N(t)$ représentant les montants de réclamations ;
	\item $\delta$ est la force d'intérêt appliquée pour actualiser les réclamations.
\end{itemize}

\item Dans un processus de renouvellement ordinaire, on a, pour $k = 1, 2, ..., n$,
\begin{align*}
f_{S_k | N(t)}(x | n) = f_{S_k}(x) \frac{\prob{N(t-x) = n-k}}{\prob{N(t) = n}}
\end{align*}

\item On peut calculer le premier moment du processus des réclamations escomptées $\{ Z(t) ; t \geq 0 \}$ : 
\[\esp{Z(t)} = \esp{X} \int_{0}^{t} e^{-\delta x} d m(x) \]
où $m(t)$ est la fonction de renouvellement du processus de renouvellement $\{ N(t) ; t \geq 0 \}$.
\end{itemize}

\section{Mouvement Brownien}
\subsection*{Définitions}
\begin{definition}[Définition générale]
Un processus stochastique $\{ X(t) ; t \geq 0 \}$ est dit être un mouvement Brownien \emph{avec paramètre de variance} $\sigma^2$ si
\begin{enumerate}[label=(\arabic*)]
\item $X(0) = 0$ ;
\item $\{ X(t) ; t \geq 0 \}$  a des accroissements indépendants et stationnaires ;
\item $\forall t > 0$, $X(t) \sim \mathcal{N}(0, \sigma^2 t)$.
\end{enumerate}
\end{definition}
Note : on appelle aussi $\sigma$ le \emph{paramètre de volatilité} ou \emph{coefficient de diffusion}. Un mouvement Brownien est dit \emph{standard} si $\sigma = 1$.

\subsubsection*{Proposition 1}
Considérons un mouvement Brownien standard. Alors, $\forall 0 < t_1 < t_2 < ... < t_{n}$, on a
\begin{align*}
f_{X_1(t_1), ..., X_n(t_n)}(x_1, ..., x_n) = \frac{e^{-\frac{1}{2} \left(\frac{x_1^2}{t_1} + \frac{(x_2 - x_1)^2}{t_2 - t_1} + ... + \frac{(x_n - x_{n-1})^2}{t_n - t_{n-1}} \right)}}{(2 \pi)^{\frac{n}{2}} (t_1 (t_2 - t_2) ... (t_n - t_{n-1}))^{\frac{1}{2}}}
\end{align*}

\subsubsection*{Proposition 2}
Considérons un mouvement Brownien standard. Alors, $\forall 0 < s < t$, $X(s) | X(t)$ obéit à une loi normale, tel que
\begin{align*}
\esp{X(s) | X(t) = x}&  = \frac{s}{t} x \\
\variance{X(s) |X(t) = x} & = \frac{s}{t} \left( t - s \right) \\
\end{align*}

\subsection*{Temps d'atteinte d'une barrière}
\begin{itemize}
\item Soit $T_a$ le le premier moment où le mouvement Brownien standard atteint le niveau $a$. Alors,
\[\prob{T_a \leq t} = \sqrt{\frac{2}{\pi}} \int_{|a| / \sqrt{t}}^{\infty} e^{- \frac{x^2}{2}} dx   \]

\item On peut trouver la distribution de la valeur maximale que peut prendre $\{ X(s) ; 0 \leq s \leq t \}$, telle que
\[\prob{\max_{0 \leq s \leq t} X(s) \geq a} =  \sqrt{\frac{2}{\pi}} \int_{a / \sqrt{t}}^{\infty} e^{- \frac{x^2}{2}} dx\]
\end{itemize}


\subsection*{Variations sur le mouvement Brownien}
\subsubsection*{Mouvement Brownien avec dérive}
Un mouvement Brownien avec dérive (\emph{drifted}) a exactement la même définition qu'un mouvement Brownien standar, à l'exception que
\[X(t) \sim \mathcal{N} \left(\mu t, \sigma^2 t \right) \]
où $\mu$ est le \emph{paramètre de dérive}. Note : on a donc que $X(t) = \mu t + \sigma B(t)$, où $B(t)$ est un mouvement Brownien standard.

\subsubsection*{Mouvement Brownien géométrique}
\begin{definition}[Définition]
Soit $\{X(t) ; t \geq 0 \}$ un mouvement Brownien brownien avec dérive $\mu$ et volatilité $\sigma$. Alors, le processus $\{X(t) ; t \geq 0 \}$ défini par
\[X(t) = e^{Y(t)}\]
est dit être un mouvement Brownien \emph{géométrique}.
\end{definition}
\textbf{Proposition : } Soit $\{X(t) ; t \geq 0 \}$ un mouvement Brownien géométrique avec dérive $\mu$ et volatilité $\sigma$. Alors,
\[\esp{X(t) | X(u)} = X(s) e^{(t-s) \left(\mu + \frac{\sigma^2}{2} \right)} \]
pour $ 0 \leq u \leq s \leq t$.

\subsubsection*{Pont Brownien}
\begin{definition}[Processus Gaussien]
Un processus stochastique $\{X(t) ; t \geq 0 \}$ est dit être un processus Gaussien si, $\forall 0 < t_1 < t_2 < ... < t_n$, $X(t_1), ..., X(t_n)$ a une distribution normale multivariée.
\end{definition}

\begin{definition}[Définition alternative d'un mouvement Brownien standard]
Un processus $\{X(t) ; t \geq 0 \}$ est un mouvement Brownien standard ssi
\begin{enumerate}[label=(\arabic*)]
\item $\{X(t) ; t \geq 0 \}$ est un processus Gaussien ;
\item $\forall t > 0$, $\esp{X(t)} = 0$, avec $X(0) = 0$ ;
\item $\forall 0 \leq s \leq t$, on a $\covar{X(s), X(t)} = s$.
\end{enumerate}
\end{definition}

\begin{definition}[Définition d'un pont Brownien]
Soit $\{X(t) ; t \geq 0 \}$ un mouvement Brownien standard. Alors, le processus conditionnel $\{X(t) ; 0 \leq t \leq 1 | X(1) = 0 \}$ est dit être un \emph{pont} Brownien.
\end{definition}














\end{multicols*}

%% -----------------------------
%% Fin du document
%% -----------------------------
\end{document}